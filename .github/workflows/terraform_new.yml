# .github/workflows/terraform.yml

name: Terraform Kubeadm Deployment (Manual)

on:
  workflow_dispatch: # This allows manual triggering from the GitHub Actions UI
    inputs:
      action_type:
        description: 'Terraform action to perform'
        required: true
        default: 'plan'
        type: choice
        options:
          - plan
          - apply
          - destroy
      target_branch:
        description: 'Branch to run Terraform on'
        required: true
        default: 'main'
        type: string
      confirm_destroy:
        description: 'Type "destroy" to confirm infrastructure destruction for destroy action'
        required: false
        default: ''
        type: string

env:
  AWS_REGION: ${{ vars.AWS_REGION || 'ap-south-1' }} # Use a variable if defined, otherwise default
  TF_WORKING_DIR: . # Directory containing your Terraform files

jobs:
  terraform:
    name: 'Terraform'
    runs-on: ubuntu-latest
    environment: production # Or 'dev' for development/testing environment
    permissions:
      contents: read
      pull-requests: write # Still useful if you want to generate PR comments on a 'plan'
      id-token: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.inputs.target_branch }} # Checkout the specified branch

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.x.x

      - name: Terraform Init
        id: init
        run: terraform init -backend-config="bucket=${{ secrets.TF_STATE_BUCKET_NAME }}" -backend-config="key=${{ env.TF_WORKING_DIR }}.tfstate" -backend-config="region=${{ env.AWS_REGION }}"
        working-directory: ${{ env.TF_WORKING_DIR }}

      - name: Terraform Format and Check
        id: format
        run: terraform fmt -recursive # This will format the files. If it's still incorrect, it means there's a deeper syntax issue.
        working-directory: ${{ env.TF_WORKING_DIR }}

      - name: Terraform Plan
        id: plan
        # Run plan only if action_type is 'plan' or 'apply'
        if: github.event.inputs.action_type == 'plan' || github.event.inputs.action_type == 'apply'
        run: terraform plan -no-color -out=tfplan
        working-directory: ${{ env.TF_WORKING_DIR }}
        continue-on-error: true # Allow plan to fail without stopping the workflow

      - name: Terraform Apply
        # Run apply only if action_type is 'apply'
        if: github.event.inputs.action_type == 'apply'
        run: terraform apply -auto-approve tfplan # Apply the previously generated plan
        working-directory: ${{ env.TF_WORKING_DIR }}

      - name: Terraform Destroy
        # Run destroy only if action_type is 'destroy' AND confirmation matches
        if: github.event.inputs.action_type == 'destroy' && github.event.inputs.confirm_destroy == 'destroy'
        run: terraform destroy -auto-approve
        working-directory: ${{ env.TF_WORKING_DIR }}

      - name: Get Master Node Public IP
        # Only run this if apply was successful
        if: success() && github.event.inputs.action_type == 'apply'
        id: get_ip
        run: echo "MASTER_IP=$(terraform output -raw master_public_ip)" >> $GITHUB_OUTPUT
        working-directory: ${{ env.TF_WORKING_DIR }}

      - name: Display Kubeconfig Instructions
        # Only display instructions if apply was successful
        if: success() && github.event.inputs.action_type == 'apply'
        run: |
          echo "## Kubeconfig Retrieval and Worker Node Join Instructions"
          echo "---"
          echo "Connect to the Master Node via SSH:"
          echo "ssh -i ~/.ssh/${{ vars.cluster_name || 'my-kubeadm-lab'}}-key.pem ubuntu@${{ steps.get_ip.outputs.MASTER_IP }}"
          echo ""
          echo "Once SSH'd into the Master Node:"
          echo "1. Get kubeconfig: sudo cat /etc/kubernetes/admin.conf > ~/kubeconfig"
          echo "2. Copy kubeconfig to your local machine (from your local machine):"
          echo "   scp -i ~/.ssh/${{ vars.cluster_name || 'my-kubeadm-lab'}}-key.pem ubuntu@${{ steps.get_ip.outputs.MASTER_IP }}:~/kubeconfig ."
          echo "3. Set KUBECONFIG: export KUBECONFIG=./kubeconfig"
          echo "4. Test: kubectl get nodes"
          echo ""
          echo "---"
          echo "To Join Worker Nodes:"
          echo "1. Get join command from Master: sudo cat /home/ubuntu/kubeadm_join_command.sh"
          echo "2. SSH into each Worker Node (private IPs are outputted by Terraform):"
          echo "   Example: ssh -i ~/.ssh/${{ vars.cluster_name || 'my-kubeadm-lab'}}-key.pem ubuntu@<WORKER_PRIVATE_IP>"
          echo "3. Run the join command (as sudo): sudo <PASTED_JOIN_COMMAND>"
          echo "---"