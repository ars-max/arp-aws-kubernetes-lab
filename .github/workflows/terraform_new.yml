# .github/workflows/terraform.yml

name: Terraform Kubeadm Deployment

on:
  push:
    branches:
      - main # Trigger on pushes to the main branch
    paths:
      - '**.tf'
      - 'user-data-*.sh' # Trigger if user data scripts change
  pull_request:
    branches:
      - main # Trigger on pull requests to the main branch
    paths:
      - '**.tf'
      - 'user-data-*.sh'
  workflow_dispatch: # Allows manual triggering
    inputs:
      destroy:
        description: 'Set to true to destroy infrastructure'
        required: false
        default: 'false'

env:
  AWS_REGION: ${{ vars.AWS_REGION || 'us-east-1' }} # Use a variable if defined, otherwise default
  TF_WORKING_DIR: . # Directory containing your Terraform files

jobs:
  terraform:
    name: 'Terraform'
    runs-on: ubuntu-latest
    environment: production # Or 'dev' for development/testing environment
    permissions:
      contents: read
      pull-requests: write
      id-token: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.x.x

      - name: Terraform Init
        id: init
        run: terraform init -backend-config="bucket=${{ secrets.TF_STATE_BUCKET_NAME }}" -backend-config="key=${{ env.TF_WORKING_DIR }}.tfstate" -backend-config="region=${{ env.AWS_REGION }}"
        working-directory: ${{ env.TF_WORKING_DIR }}

      - name: Terraform Format
        id: format
        run: terraform fmt -check
        working-directory: ${{ env.TF_WORKING_DIR }}

      - name: Terraform Plan
        id: plan
        if: github.event_name == 'pull_request' && github.event.inputs.destroy != 'true'
        run: terraform plan -no-color -out=tfplan
        working-directory: ${{ env.TF_WORKING_DIR }}
        continue-on-error: true

      - name: Update Pull Request with Terraform Plan
        uses: actions/github-script@v6
        if: github.event_name == 'pull_request' && github.event.inputs.destroy != 'true'
        with:
          script: |
            const output = `#### Terraform Plan ðŸ“–
            \`\`\`terraform
            ${process.env.PLAN_OUTPUT}
            \`\`\`
            `;
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: output
            })
           env:
             PLAN_OUTPUT: ${{ steps.plan.outputs.stdout }}

      - name: Terraform Apply
        if: github.event_name == 'push' && github.ref == 'refs/heads/main' && github.event.inputs.destroy != 'true'
        run: terraform apply -auto-approve
        working-directory: ${{ env.TF_WORKING_DIR }}

      - name: Terraform Destroy
        if: github.event_name == 'workflow_dispatch' && github.event.inputs.destroy == 'true'
        run: terraform destroy -auto-approve
        working-directory: ${{ env.TF_WORKING_DIR }}

      - name: Get Master Node Public IP
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        id: get_ip
        run: echo "MASTER_IP=$(terraform output -raw master_public_ip)" >> $GITHUB_OUTPUT
        working-directory: ${{ env.TF_WORKING_DIR }}

      - name: Display Kubeconfig Instructions
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          echo "## Kubeconfig Retrieval and Worker Node Join Instructions"
          echo "---"
          echo "Connect to the Master Node via SSH:"
          echo "ssh -i ~/.ssh/${{ vars.cluster_name || 'my-kubeadm-lab'}}-key.pem ubuntu@${{ steps.get_ip.outputs.MASTER_IP }}"
          echo ""
          echo "Once SSH'd into the Master Node:"
          echo "1. Get kubeconfig: sudo cat /etc/kubernetes/admin.conf > ~/kubeconfig"
          echo "2. Copy kubeconfig to your local machine (from your local machine):"
          echo "   scp -i ~/.ssh/${{ vars.cluster_name || 'my-kubeadm-lab'}}-key.pem ubuntu@${{ steps.get_ip.outputs.MASTER_IP }}:~/kubeconfig ."
          echo "3. Set KUBECONFIG: export KUBECONFIG=./kubeconfig"
          echo "4. Test: kubectl get nodes"
          echo ""
          echo "---"
          echo "To Join Worker Nodes:"
          echo "1. Get join command from Master: sudo cat /home/ubuntu/kubeadm_join_command.sh"
          echo "2. SSH into each Worker Node (private IPs are outputted by Terraform):"
          echo "   Example: ssh -i ~/.ssh/${{ vars.cluster_name || 'my-kubeadm-lab'}}-key.pem ubuntu@<WORKER_PRIVATE_IP>"
          echo "3. Run the join command (as sudo): sudo <PASTED_JOIN_COMMAND>"
          echo "---"
        # Using echo to print to action logs, which are accessible from GitHub UI
        # You can also use `actions/upload-artifact` to upload a file with these instructions.
